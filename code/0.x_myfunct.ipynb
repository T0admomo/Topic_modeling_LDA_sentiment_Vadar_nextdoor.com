{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e92471-969d-4e5d-9d74-6a63dec6285c",
   "metadata": {},
   "source": [
    "## Read and Write Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b969594-4127-41d5-a5cc-bc87cf9f1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import json\n",
    "import glob\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from spacy.tokens import DocBin\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "import spacy\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0957f4-8d52-4ee0-ae50-a99c67c6298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open (file, 'r',encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def write_json(file, data):\n",
    "    with open (file, 'w',encoding = 'utf-8') as f:\n",
    "        json.dump(data, f, indent = 4)\n",
    "\n",
    "def load_pkl(file):\n",
    "    with open(file,'rb') as raw_pickle:\n",
    "         data = pickle.load(raw_pickle)\n",
    "    return data\n",
    "\n",
    "def write_pkl(file, data):\n",
    "    with open(file,'wb') as raw_pickle:\n",
    "         pickle.dump(data, raw_pickle) \n",
    "            \n",
    "            \n",
    "            \n",
    "def load_data(version):   \n",
    "    \"\"\"\n",
    "    Takes a file name and \n",
    "    returns token_texts, corpus, id2word\n",
    "    \"\"\"\n",
    "    \n",
    "    token_text = load_json(f'../data/tokens/{version}.json')\n",
    "\n",
    "    corpus = load_json(f'../data/corpi/{version}.json')\n",
    "\n",
    "    id2word = load_pkl(f'../data/word_ids/{version}.pkl')\n",
    "    \n",
    "    return token_text, corpus, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573f880-179e-4538-b3eb-3fc59f765ccc",
   "metadata": {},
   "source": [
    "# Tf-IDF REMOVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6314d-f603-4d6c-939b-ed234dfd0963",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419f683-83a3-4c82-a55a-e83684b29883",
   "metadata": {},
   "source": [
    "We arn't going to want to move forward with this kind of word removal until we have done more manual inspection and stop word removal. The brevity of our texts may demand that we keep as many words as possible, and lossing frequently occurring words seminal to the focus of the study .i.e homelessness would prevent us from identifying a posts relevancy overall at this time. for example. we still do not currently know if all of our posts pertain to homelessness, or if they include many discussions of cats up tree's. Untill we have done more cleaning, we will want our topic model to to isolate irrelavant posts, removing \"homeless\" from all of the posts would make it rather difficult to do this. So it is a step better saved for latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a8ef968-151c-4e0a-bbd3-fc26c75ba6cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from gensim.models import TfidfModel\n",
    "\n",
    "# # create word dictionary \n",
    "# id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "# # just to make it simpler going forward \n",
    "# texts = data_bigrams_trigrams\n",
    "# # convert all of our texts into a bag of words\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# print ( corpus[0][0:20])\n",
    "\n",
    "# # instantiate tfidf model \n",
    "# tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "# low_value = 0.03\n",
    "# words = []\n",
    "# words_missing_in_tfidf = []\n",
    "\n",
    "\n",
    "# for i in range(0, len(corpus)):\n",
    "#     bow = corpus[i]\n",
    "#     low_value_words = [] \n",
    "#     tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "#     bow_ids = [id for id, value in bow]\n",
    "#     low_value_words = [id for id, value in tfidf[bow] if value < low_value] \n",
    "#     drops = low_value_words+words_missing_in_tfidf\n",
    "#     for item in drops:\n",
    "#         words.append(id2word[item])\n",
    "#     words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # the words with tf-idf score will be missing\n",
    "    \n",
    "#     new_bow= [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "#     corpus[i] = new_bow\n",
    "    \n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79662e24-d877-46b9-a828-8ff5c721d309",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8f6bed-3be5-43c5-bcdd-c9bffbffd30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sim_word(kwds, top_n ):\n",
    "    \"\"\"\n",
    "    kwds : list of search terms\n",
    "    top_n: number of similar term to return \n",
    "    \n",
    "    returns: top_n most similar words for each of the terms in kwds\n",
    "    \"\"\"\n",
    "    \n",
    "    for kwd in kwds:\n",
    "        try:\n",
    "            print(kwd)\n",
    "            res = model.wv.similar_by_word(kwd, topn=top_n)\n",
    "            for term in res:\n",
    "                print(term)\n",
    "        except KeyError:\n",
    "            print('[]\"Word not in vocabulary\"[]]')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd564a13-b5c2-4957-8363-f235fb56365f",
   "metadata": {},
   "source": [
    "## Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b46fd9-08b6-4ec1-bfea-7641e2687ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postages=['NOUN','ADJ','VERB','ADV']):\n",
    "    \n",
    "    '''\n",
    "    allowed_postage : the parts of speach we want to keep [DEFAULT: 'NOUN','ADJ','VERB','ADV'] \n",
    "    '''\n",
    "    \n",
    "          # load in spacy sm web model \n",
    "    nlp = spacy.load('en_core_web_lg', diasble = ['parser','ner']) # computaltionally expensize aspects \n",
    "    texts_out = [] # output\n",
    "    \n",
    "    # for each post in the corpus\n",
    "    # iterate over texts\n",
    "    for text in texts:  \n",
    "        # creates spacy doc object containing vectorized contextual information like Parts of Speech (pos) \n",
    "        doc = nlp(text)\n",
    "                \n",
    "        # list for holding lemmatized tokens\n",
    "        new_text = []\n",
    "        # iterate over each token\n",
    "        for token in doc:\n",
    "            # only keep the desired pos\n",
    "            if token.pos_ in allowed_postages:\n",
    "                # cleans minimal stop words and punctuation \n",
    "                if not token.is_stop == True:\n",
    "                    # reducing model complexity by reducing tokens to lemma_ \n",
    "                    new_text.append(token.lemma_)   \n",
    "                    # print(token.lemma_)\n",
    "\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "\n",
    "def gen_words(texts, stopwords= None ):\n",
    "    \n",
    "    final = [] \n",
    "    for text in texts:\n",
    "        # Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "        temp = gensim.utils.simple_preprocess(text , deacc=True)# – Remove accent marks from tokens using \n",
    "        new = []\n",
    "        for word in temp:\n",
    "            if word not in stop_words:\n",
    "                new.append(word)\n",
    "        #new = [~ t for t in new if t in stop_words]\n",
    "        final.append(new)\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff032e-d770-4428-b282-5f3d367566bc",
   "metadata": {},
   "source": [
    "## Word Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0a057e-9bcc-4bde-957b-1315ff1898e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "# https://spacy.io/usage/training#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce71d87-a103-45f0-8da5-84384b001024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = [] \n",
    "    for text in texts:\n",
    "        # new = mwe.tokenize(text)\n",
    "        # Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "        new = gensim.utils.simple_preprocess(text , deacc=True)# – Remove accent marks from tokens using \n",
    "        final.append(new)\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcdd9f-b081-4824-b5c6-f55ecd0a962e",
   "metadata": {},
   "source": [
    "# Bigrams &  Trigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18f233-436c-42c2-93a3-83b5eaee5e89",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205527e-288e-4fb6-a00d-d4f33b9f98ac",
   "metadata": {},
   "source": [
    "We attempt to capture some of the more important word pairings with bigrams and trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbe3b9a-e95e-4150-a90c-02fdeeade8c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=UEn3xHNBXJU\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#9createbigramandtrigrammodels\n",
    "\n",
    "## set parameters\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "# # Build the bigram and trigram models\n",
    "# bigram_model = gensim.models.Phrases(data_words, min_count = 3, threshold = 50 )# min freq for a coupling to be a bigram ## thresh = num of bigrams allowes\n",
    "# # of the bigrams, are is their overlap in the rest of our words for a trigram?\n",
    "# trigram_model = gensim.models.Phrases(bigram_model[data_words], threshold = 50 )\n",
    "\n",
    "## create \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "\n",
    "# # fit bigram model \n",
    "# bigram = gensim.models.phrases.Phraser(bigram_model)\n",
    "# trigram = gensim.models.phrases.Phraser(trigram_model)\n",
    "\n",
    "# # instantia\n",
    "# data_bigrams = make_bigrams(data_words)\n",
    "# data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "# print(data_bigrams_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39a0c8-0777-444d-8032-89f5c585503c",
   "metadata": {},
   "source": [
    "# Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524959db-5e7d-4d62-acf2-5eb41911e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "from gensim.models import CoherenceModel \n",
    "\n",
    "def compute_coherence_values(corpus, text_tokens, id2word, k, a, b, c ):\n",
    "    \n",
    "    \"\"\"\n",
    "    corpus: text body in string form\n",
    "    dictionary: id2word \n",
    "    k: num_topics\n",
    "    a: alpha - document topic density\n",
    "    b: beta- word topic density \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=c,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text_tokens, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e81402-d484-4a55-a6aa-f66de5ddb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lda(corpus, text_tokens, id2word, t_range, c_range, a_range, b_range ):\n",
    "    # Can take a long time to run\n",
    "    # result df entry format\n",
    "    model_results = {'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Chunks': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    # iterate through number of topics\n",
    "    for k in t_range:\n",
    "        # iterate through alpha values\n",
    "        for a in a_range:\n",
    "              # iterare through beta values\n",
    "            for b in b_range:\n",
    "                # iterate through chunksizes\n",
    "                for c in c_range:\n",
    "                     # get cohenerence value\n",
    "\n",
    "                    cv = compute_coherence_values( corpus, text_tokens, id2word, \n",
    "                                                  k, a, b, c)\n",
    "                    # Save the model results\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Chunks'].append(c)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    return pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5b8fb-95d7-4eab-a5f0-bd9097cb9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_similarity(word, filepath):\n",
    "    model = KeyedVectors.load_word2vec_format(f'{filepath}', binary = False)\n",
    "    results = model.most_similar(positive =[word])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8c1e731-1eea-404c-8dd5-f47fdf513fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sim_word(kwds, top_n):\n",
    "    \"\"\"\n",
    "    kwds : list of search terms\n",
    "    top_n: number of similar term to return \n",
    "    \n",
    "    returns: top_n most similar words for each of the terms in kwds\n",
    "    \"\"\"\n",
    "    # for each search term \n",
    "    for kwd in kwds:\n",
    "        try:\n",
    "            print(kwd.upper()) # show search term \n",
    "            res = model.wv.similar_by_word(kwd, topn=top_n) # find similar words\n",
    "            for term in res:\n",
    "                print(term) # show similar words\n",
    "        except KeyError:\n",
    "            print('[]\"Word not in vocabulary\"[]]')\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b5853-76e6-47ce-a9a2-f849377be909",
   "metadata": {},
   "source": [
    "## For creating a labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00cb7a83-ee7c-43b7-bf6c-1779d1d72a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_posts(post_tokens, kwds, label):    \n",
    "    '''\n",
    "    label: class label \n",
    "    kwds: search terms\n",
    "    post_tokens: tokenized corpus\n",
    "    \n",
    "    returns: labeled dataset containing target posts\n",
    "    '''\n",
    "    \n",
    "    train_data = []   \n",
    "    match = None\n",
    "    # if a post has one of our kwds\n",
    "    for word in kwds:\n",
    "        for post in post_tokens:\n",
    "            match = False \n",
    "            # only relevant posts \n",
    "            if word in post:\n",
    "                match = True\n",
    "            # if our keyword is in a post then match = True    \n",
    "            # attaches given class label to post\n",
    "            if match == True:\n",
    "                post = ' '.join(post)\n",
    "                train_data.append((post,label))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21be6162-7fcf-4098-b206-477525bd2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import multiprocessing  # for using seperate core's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c128166-c646-436c-97f0-5392cd8900e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(model_name, filepath):\n",
    "    with open (f'{filepath}', 'r', encoding = 'utf-8') as f:\n",
    "        texts = json.load(f)\n",
    "    sentence = texts \n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(\n",
    "            min_count = 2,\n",
    "            window = 2, # num of surrounding words to consider \n",
    "            # shape of a word vector\n",
    "            size = 500,  #size of vocab , dimensions of vocab                  \n",
    "            sample = 6e-5,\n",
    "            alpha =0.03, # error term\n",
    "            min_alpha=0.0007, # ??\n",
    "            negative = 20, # ??\n",
    "            workers = cores-1 # number of cores to train with  \n",
    "        )\n",
    "    \n",
    "    w2v_model.build_vocab(texts) # create the model vocabulary \n",
    "    w2v_model.train(texts, total_examples=w2v_model.corpus_count, epochs = 30) # train the model\n",
    "    w2v_model.save(f'../data/word_vec_model_{model_name}.model') # save the model for comparison \n",
    "    w2v_model.wv.save_word2vec_format(f'../data/word_vectors_{model_name}.txt') # save word vectors\n",
    "    # https://www.youtube.com/watch?v=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201955b8-33c2-4d6c-acae-8ee34c33dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort the output probabilities\n",
    "def Sort(sub_li):\n",
    "    '''\n",
    "    takes a sub list and sorts it \n",
    "    '''\n",
    "    # sort based on the probablity value \n",
    "    sub_li.sort(key = lambda x: x[1])\n",
    "    sub_li.reverse()\n",
    "    return sub_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2c361-9346-484b-8b7d-bf58679c2c68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def output_topics(corpus, model):\n",
    "    preds = []\n",
    "    for i ,doc in enumerate(corpus):\n",
    "        # get topics for each doc\n",
    "        test_doc = corpus[i]\n",
    "        pred = model[test_doc]\n",
    "        # sort prediction probablitities\n",
    "        new_pred = Sort(pred)\n",
    "        # return highest probability \n",
    "        #preds.append(new_pred)\n",
    "        preds.append(new_pred[0][0])\n",
    "    return preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('work': conda)",
   "language": "python",
   "name": "python382jvsc74a57bd08696d3c135d391e67f0eae834eacd73bc5019c07a594ec3c835ec49a48ec6a4c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
