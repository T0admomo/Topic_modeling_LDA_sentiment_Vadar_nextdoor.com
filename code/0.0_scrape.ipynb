{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d9895-1862-403f-99ad-692e236e1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813af748-fce3-4afe-a86a-576aa2e01f12",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "___\n",
    "Next door requires users to register their address, and allows them to view posts of other based on geographic location. Those seeking to replicate this project will need a Nextdoor account in the region they wish to study, and will need to provide their own account information in the code below.\n",
    "\n",
    "Next door is intended as a community social network, and much of the content pertains to city or neighborhood wide news, community events, or pleas to pick up pet droppings from someones favorite walking trail. However the site's user based geo-restrictions create a unique virtual environment where users can feel safe that their post's wont travel far outside of their communities. This has the capacity to effect the tone and types of conversations users have whether that be for the better or worse, and presents itself as a social space ripe for study. \n",
    "\n",
    "Use of Nextdoor content also allows us to cut through much of the noise that we would typically obtain when collecting data for the study of sentiments by geographic region. On sites like twitter , or facebook, users geo-graphic info is often incorrect or intentionally misleading, and users from one area posting on groups from another and other similar cases can quickly obscure meaningful results. \n",
    "\n",
    "Collecting data from Nextdoor has it's own challenges though. Without an API we were left to web-scrapping. I used selenium for it's ability to interact with Nextdoor.com's JS heavy webpages. The reduced scrapping speed of selenium is actually comes with the benifit of allowing us to lightly monitor the incoming data giving us time to get familiar with the data. \n",
    "\n",
    "This study was conducted on media content originating from over a dozen neighborhoods in the east Denver Area.\n",
    "\n",
    "|posts|comments|\n",
    "|---|---|\n",
    "|1,100|16,000|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7a8b9-30ef-4849-be55-efc16066fbea",
   "metadata": {},
   "source": [
    "#  Building A Web Scrapper\n",
    "___\n",
    "___\n",
    "Below are the functions used to scrape nextdoor.com for content using the sites search function. I had difficulties putting this into a package that , if you are reading this an know how to solve, I would welcome you to submit a pull request. Next door offers a unique insight into neighborhood sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4c07f-955e-4b5c-af74-298f2106e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nxtdr_logout():\n",
    "    driver.get('https://nextdoor.com/logout/?ucl=1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nxtdr_login(usrnm, pwd):\n",
    "    \n",
    "    \n",
    "\n",
    "    time.sleep(2)\n",
    "    log_in = driver.find_element_by_class_name(\"css-1d8yfou\")\n",
    "        # click log in button \n",
    "    log_in.click()\n",
    "\n",
    "        # enter email \n",
    "    email = driver.find_element_by_id('id_email')\n",
    "    email.send_keys(usrnm)\n",
    "    email.send_keys(Keys.TAB)\n",
    "\n",
    "        # enter pwd\n",
    "    enter_pwd = driver.find_element_by_id(\"id_password\")\n",
    "    enter_pwd.send_keys(pwd)\n",
    "        # log in \n",
    "    enter_pwd.send_keys(Keys.RETURN)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query(keywords):\n",
    "    ''' Searches Keyword on ND\n",
    "        Must be logged in to run \n",
    "    '''\n",
    "    url = 'https://nextdoor.com/'\n",
    "    driver.get(f'{url}search/?query={keywords}')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def search( keywords, reps ):\n",
    "    query(keywords)\n",
    "    time.sleep(10)\n",
    "    load_more(reps)\n",
    "    links = get_links()\n",
    "    return links\n",
    "get_ipython().run_line_magic('time', '')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_more(reps):\n",
    "\n",
    "    # click the load more button at the bottom of the page # of times in range(reps)'''\n",
    "\n",
    "    for i in range(reps):\n",
    "        try:\n",
    "            show_more = driver.find_element_by_class_name('css-1on4yel')\n",
    "            show_more.click()\n",
    "            # allowing time for page to load\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "\n",
    "            show_more = driver.find_element_by_class_name('content-results-list-item-see-more-link')\n",
    "            show_more.click()\n",
    "            # allowing time for page to load\n",
    "            time.sleep(3)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07b17b-1ce1-4f9c-ba1f-c1a4f4616c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def scrape(usrnm, pwd, keywords):\n",
    "    nxtdr_login(usrnm,pwd)\n",
    "    time.sleep(3)\n",
    "    links = search(keywords,100)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    links_df = pd.DataFrame()\n",
    "     # fill out href column with contents of links list \n",
    "    links_df['href'] = [ link for link in links]\n",
    "     # clean href \n",
    "    links_df['href'] = [ href[0] for href in links_df['href'].str.split('&')]\n",
    "    # drop duplicates posts\n",
    "    links_df.drop_duplicates( inplace = True,   keep='first')\n",
    "    # only take user posts not ads etc.\n",
    "    usr_post = links_df['href'].str.contains(' ?post=')\n",
    "    links_df = links_df[usr_post]\n",
    "    return links_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_links():\n",
    "    \n",
    "    # all links on webpage ( web element )\n",
    "    feed = driver.find_elements_by_tag_name('a')\n",
    "\n",
    "    links = []\n",
    "\n",
    "        # the first five links are advertisements\n",
    "    for post in feed[5:]:\n",
    "        link = post.get_attribute('href')\n",
    "        links.append(link)\n",
    "    # returns list of links from full loaded webpage resulting \n",
    "    # from keyword search.  \n",
    "    return links\n",
    "\n",
    "\n",
    "def get_post(href):\n",
    "    # open post in browser\n",
    "    driver.get(href)\n",
    "           # give time to load\n",
    "    time.sleep(3)\n",
    "        # container for post and comments \n",
    "    post_container = driver.find_element_by_class_name('css-1dkvlfs')\n",
    "        # container for post\n",
    "    main_post_container = post_container.find_element_by_class_name('cee-media-body')\n",
    "        # actual text of main post \n",
    "    main_post = main_post_container.find_element_by_class_name('Linkify').text\n",
    "        # post id \n",
    "    post_id = href.split('=')[1]\n",
    "\n",
    "    # meta info\n",
    "        # location\n",
    "    main_post_location = main_post_container.find_element_by_tag_name('button').text\n",
    "        # author name\n",
    "    meta = main_post_container.find_elements_by_tag_name('a')\n",
    "    main_post_author = meta[0].text\n",
    "\n",
    "        # post date \n",
    "        # list of entities that mess up date pull \n",
    "    date_fix_list = ['City of Denver','News','Denver Police Department']\n",
    "\n",
    "    if main_post_author in date_fix_list:\n",
    "        main_post_date = meta[2].text\n",
    "    else:\n",
    "        main_post_date = meta[1].text\n",
    "\n",
    "        # post to append to post_df\n",
    "    post = {'post_id': post_id ,\n",
    "           'author' : main_post_author, 'date': main_post_date,\n",
    "           'location': main_post_location, 'post': main_post}\n",
    "    \n",
    "    return post\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_comments(href):\n",
    "        # container for post and comments \n",
    "    post_container = driver.find_element_by_class_name('css-1dkvlfs')\n",
    "        # container for post\n",
    "    main_post_container = post_container.find_element_by_class_name('cee-media-body')\n",
    "        # the boxes around each of the comments on the post\n",
    "    comment_windows = post_container.find_elements_by_class_name('js-media-comment')\n",
    "        # post id \n",
    "    \n",
    "    \n",
    "        # creating a list of comments\n",
    "    comments = []\n",
    "    for i in range(len(comment_windows)):\n",
    "        comment = comment_windows[i].find_element_by_class_name('css-1srqc6z').text\n",
    "        comment = comment.split('\\n')\n",
    "        comments.append(comment[1])\n",
    "        \n",
    "        # creating a list of locations\n",
    "    locations = []\n",
    "    for i in range(len(comment_windows)):\n",
    "        location = comment_windows[i].find_element_by_class_name('comment-detail-scopeline').text\n",
    "        locations.append(location)\n",
    "        \n",
    "        \n",
    "        # creating a list of authors \n",
    "    authors = []\n",
    "    for i in range(len(comment_windows)):\n",
    "        author = comment_windows[i].find_element_by_class_name('author-menu-box-container').text\n",
    "        authors.append(author)\n",
    "        comments_df = pd.DataFrame(columns = ['post_id','author','location','date','comment'])\n",
    "    \n",
    "    \n",
    "        # creating list of dates\n",
    "    dates = []\n",
    "    comment_dates = driver.find_elements_by_class_name('css-9p9z55')\n",
    "    for comment in comment_dates:\n",
    "        dates.append(comment.text)\n",
    "        \n",
    "    post_ids= []\n",
    "    for i in range(len(comment_windows)):\n",
    "        post_id = href.split('=')[1]\n",
    "        post_ids.append(post_id)\n",
    "  \n",
    "    comments_df = pd.DataFrame()\n",
    "        # set columns of dataframe\n",
    "    comments_df['post_id'] = [post_id for post_id in post_ids]\n",
    "    comments_df['location'] = [loc for loc in locations]\n",
    "    comments_df['date'] = [date for date in dates]\n",
    "    comments_df['author'] = [auth for auth in authors] \n",
    "    comments_df['comment'] = [com for com in comments]\n",
    "\n",
    "    return comments_df\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "def get_content(df):   \n",
    "    # PATH = './chromedriver.exe'\n",
    "    # driver = webdriver.Chrome(PATH)\n",
    "    \n",
    "    posts_df = pd.DataFrame(columns = ['post_id','author','location','date','post'])\n",
    "    comments_df = pd.DataFrame(columns = ['post_id','author','location','date','comment'])\n",
    "    comments_master_df = pd.DataFrame(columns = ['post_id','author','location','date','comment'])\n",
    "    \n",
    "    for href in df['href']:\n",
    "        #comments_temp_df = pd.DataFrame(columns = ['post_id','author','location','date','comment'])\n",
    "\n",
    "        try:\n",
    "            post = get_post(href)\n",
    "            posts_df = posts_df.append(post, ignore_index=True)\n",
    "            comments_df = get_comments(href)\n",
    "            comments_master_df = comments_master_df.append(comments_df)\n",
    "        except: \n",
    "            continue\n",
    "    return posts_df, comments_master_df   \n",
    "\n",
    "\n",
    "\n",
    "def run_scrape():\n",
    "    print('Ready To Scrape Nextdoor!')\n",
    "    usr_email = input('What is your email:')\n",
    "    pwd = input('Enter Pwd:')\n",
    "    keyword = input('Input Search Term:')\n",
    "    \n",
    "    href_list = scrape(f'{usr_email}', f'{pwd}', f'{keyword}' )\n",
    "    posts, comments = get_content(href_list)\n",
    "    \n",
    "    return posts, comments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb6776-1ab0-4376-bff0-ce77ad77e64d",
   "metadata": {},
   "source": [
    "# Web Scrapper V2\n",
    "___\n",
    "___\n",
    " \n",
    "It appeared during my scrape that the nextdoor.com server noticed my activity and began responding with altered HTML and URL formats. This may have also been due to routine maintenance on the server. In either case, here is the code that was written to get around the changes when they were encountered. This will return a dataframe where the post_id is in a different format than the previous version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995204ea-514c-44f3-b498-c96b314c9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def scrape_2(usrnm, pwd):\n",
    "    '''\n",
    "    get links for given key word'''\n",
    "    nxtdr_login(usrnm,pwd)\n",
    "    time.sleep(3)\n",
    "    links = search('homeless',100)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    links_df = pd.DataFrame()\n",
    "    # fill out href column with contents of links list \n",
    "    links_df['href'] = [ link for link in links]\n",
    "     # clean href \n",
    "    links_df['href'] = [ href[0] for href in links_df['href'].str.split('?')]\n",
    "    # drop duplicates posts\n",
    "    links_df.drop_duplicates( inplace = True,   keep='first')\n",
    "    # only take user posts not ads etc.\n",
    "    usr_post = links_df['href'].str.contains(f'/p/')\n",
    "    links_df = links_df[usr_post]\n",
    "    return links_df\n",
    "\n",
    "\n",
    "\n",
    "def get_post_2(href):\n",
    "    '''\n",
    "    scrape post from given link \n",
    "    '''\n",
    "    # open post in browser\n",
    "    driver.get(href)\n",
    "           # give time to load\n",
    "    time.sleep(1)\n",
    "        # container for post and comments \n",
    "   # post_container = driver.find_element_by_class_name('css-1dkvlfs')\n",
    "        # container for post\n",
    "    main_post_container = driver.find_element_by_class_name('cee-media-body')\n",
    "        # actual text of main post \n",
    "    main_post = main_post_container.find_element_by_class_name('Linkify').text\n",
    "        # post id \n",
    "    post_id = href.split('/p/')[1]\n",
    "\n",
    "    # meta info\n",
    "        # location\n",
    "    main_post_location = main_post_container.find_element_by_tag_name('button').text\n",
    "        # author name\n",
    "    meta = main_post_container.find_elements_by_tag_name('a')\n",
    "    main_post_author = meta[0].text\n",
    "\n",
    "        # post date \n",
    "        # list of entities that mess up date pull \n",
    "    date_fix_list = ['City of Denver','News','Denver Police Department']\n",
    "\n",
    "    if main_post_author in date_fix_list:\n",
    "        main_post_date = meta[2].text\n",
    "    else:\n",
    "        main_post_date = meta[1].text\n",
    "\n",
    "        # post to append to post_df\n",
    "    post = {'post_id': post_id ,\n",
    "           'author' : main_post_author, 'date': main_post_date,\n",
    "           'location': main_post_location, 'post': main_post}\n",
    "    \n",
    "    return post\n",
    "\n",
    "\n",
    "\n",
    "def get_comments_2(href):\n",
    "    ''' scrape comments from given link'''\n",
    "    time.sleep(1)   \n",
    "    # clicking the seem more comments button \n",
    "    see_more_comments = driver.find_element_by_class_name('see-previous-comments-button-paged')\n",
    "    see_more_comments.click()\n",
    "   \n",
    "        # the boxes around each of the comments on the post\n",
    "    comment_window = driver.find_element_by_class_name('css-1cefqj0')\n",
    "    # tag containing authors and locations \n",
    "    comments_meta = comment_window.find_elements_by_class_name('css-15h9wih')\n",
    "    # actual text of comments\n",
    "    comments_text = comment_window.find_elements_by_class_name('_1aEnMjGe')\n",
    "    # the date of the comments\n",
    "    comments_dates = comment_window.find_elements_by_class_name('css-9p9z55')\n",
    "    \n",
    "    \n",
    "        # creating a list for all of our data types\n",
    "    post_ids, dates, authors, locations, comments = [],[],[],[],[]\n",
    "\n",
    "    # iterate through web elements and add their contents to lists \n",
    "    for comment in comments_text:\n",
    "        comments.append(comment.text)\n",
    "\n",
    "    for date in comments_dates:\n",
    "        dates.append(date.text)\n",
    "\n",
    "    for meta in comments_meta:\n",
    "        meta = meta.text.split(' • ')\n",
    "        author = meta[0]\n",
    "        location = meta[1]\n",
    "        authors.append(author)\n",
    "        locations.append(location)\n",
    "\n",
    "\n",
    "    for i in range(len(comments_dates)):\n",
    "        post_id = href.split('/p/')[1]\n",
    "        post_id = post_id.split('?')[0]\n",
    "        post_ids.append(post_id)\n",
    "      \n",
    "    # create dataframe\n",
    "    comments_df = pd.DataFrame()\n",
    "\n",
    "    # Fill values of dataframe with list values.\n",
    "    comments_df['post_id'] = [post_id for post_id in post_ids]\n",
    "    comments_df['location'] = [loc for loc in locations]\n",
    "    comments_df['date'] = [date for date in dates]\n",
    "    comments_df['author'] = [auth for auth in authors] \n",
    "    comments_df['comment'] = [com for com in comments]\n",
    "\n",
    "    return comments_df\n",
    "\n",
    "\n",
    "\n",
    "def get_content_2(df):   \n",
    "\n",
    "    posts_df = pd.DataFrame(columns = ['post_id','author','location','date','post'])\n",
    "    comments_df = pd.DataFrame(columns = ['post_id','author','location','date','comment'])\n",
    "    comments_master_df = pd.DataFrame(columns = ['post_id','author','location','date','comment'])\n",
    "    \n",
    "    for href in df['href']:\n",
    "        try:\n",
    "            post = get_post_2(href)\n",
    "            time.sleep(2)\n",
    "            posts_df = posts_df.append(post, ignore_index=True)\n",
    "            comments_df = get_comments_2(href)\n",
    "            comments_master_df = comments_master_df.append(comments_df)\n",
    "        except: \n",
    "            continue\n",
    "    return posts_df, comments_master_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b4c96-12e5-41f6-890f-089039bd5104",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# Scraping For Data w/ Selenium \n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becdabf-7314-49f7-ae27-084cc3458daa",
   "metadata": {},
   "source": [
    "I created a package for scrapping posts comments and related meta-data by search term using selenium. The code below was used to execute selenium functions from scrape.py in order to create our dataset. You will need to provide your own username and password. An up to date version of Chrome and compatible driver are also needed.\n",
    "\n",
    "This code can take up to a few hours to run once started, and will not respond to a UserAbort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c2cb1-7142-475e-92ea-700230e608ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://nextdoor.com/'\n",
    "\n",
    "# set driver\n",
    "PATH = '../chromedriver'\n",
    "driver = webdriver.Chrome(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44822b19-5b24-4690-b9fb-c6cf0e8f2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = scrape('user-email','password','homeless')\n",
    "\n",
    "p_df, c_df = get_content(user1)\n",
    "\n",
    "p_df.to_csv('../data/user_data/vol_1_posts.csv')\n",
    "c_df.to_csv('../data/user_data/vol_1_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297d3ea-e251-4659-90a2-302ad697ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nxtdr_logout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3beada-d7a1-492b-a2d4-e1d5f64d78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user2 = scrape('user-email','password','homeless')\n",
    "\n",
    "p1_df, c1_df = get_content(user2)\n",
    "\n",
    "p1_df.to_csv('../data/user_data/vol_2_posts.csv')\n",
    "c1_df.to_csv('../data/user_data/vol_2_comments.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
