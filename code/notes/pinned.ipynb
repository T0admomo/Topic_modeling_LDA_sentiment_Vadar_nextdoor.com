{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8b1b5-09e8-496f-b069-0d351f874fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim \n",
    "from gensim.utils import simple_preprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efa994d-0117-40b8-93d1-2a0f1932f19e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "final = []\n",
    "for post in posts:\n",
    "    post = simple_preprocess(post, deacc = True)\n",
    "    # filter posts less than 25 words long \n",
    "    if len(post) > 25:\n",
    "        post = [token for token in post if str(token) not in stopwords ]\n",
    "        final.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73edac15-7069-4dc7-8e55-b11276abc02d",
   "metadata": {},
   "source": [
    "# Tf-IDF REMOVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfe907-a8ef-4354-8b64-3f1fcd653a49",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05090383-6ec2-4b4f-b6a8-ae5c47c83fc7",
   "metadata": {},
   "source": [
    "We arn't going to want to move forward with this kind of word removal until we have done more manual inspection and stop word removal. The brevity of our texts may demand that we keep as many words as possible, and lossing frequently occurring words seminal to the focus of the study .i.e homelessness would prevent us from identifying a posts relevancy overall at this time. for example. we still do not currently know if all of our posts pertain to homelessness, or if they include many discussions of cats up tree's. Untill we have done more cleaning, we will want our topic model to to isolate irrelavant posts, removing \"homeless\" from all of the posts would make it rather difficult to do this. So it is a step better saved for latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b40bd78-10e5-4552-a1c8-a706e512f729",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from gensim.models import TfidfModel\n",
    "\n",
    "# # create word dictionary \n",
    "# id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "# # just to make it simpler going forward \n",
    "# texts = data_bigrams_trigrams\n",
    "# # convert all of our texts into a bag of words\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# print ( corpus[0][0:20])\n",
    "\n",
    "# # instantiate tfidf model \n",
    "# tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "# low_value = 0.03\n",
    "# words = []\n",
    "# words_missing_in_tfidf = []\n",
    "\n",
    "\n",
    "# for i in range(0, len(corpus)):\n",
    "#     bow = corpus[i]\n",
    "#     low_value_words = [] \n",
    "#     tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "#     bow_ids = [id for id, value in bow]\n",
    "#     low_value_words = [id for id, value in tfidf[bow] if value < low_value] \n",
    "#     drops = low_value_words+words_missing_in_tfidf\n",
    "#     for item in drops:\n",
    "#         words.append(id2word[item])\n",
    "#     words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # the words with tf-idf score will be missing\n",
    "    \n",
    "#     new_bow= [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "#     corpus[i] = new_bow\n",
    "    \n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a714ca8-efa3-4941-bef7-3f64ca646203",
   "metadata": {},
   "source": [
    "# Bigrams &  Trigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd823f39-672a-4a53-a010-597e50bcc517",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291ce93-4ab8-481e-a391-30a078148948",
   "metadata": {},
   "source": [
    "We attempt to capture some of the more important word pairings with bigrams and trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd4258c7-d455-4086-95a4-c2adcee170bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=UEn3xHNBXJU\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#9createbigramandtrigrammodels\n",
    "\n",
    "## set parameters\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "# # Build the bigram and trigram models\n",
    "# bigram_model = gensim.models.Phrases(data_words, min_count = 3, threshold = 50 )# min freq for a coupling to be a bigram ## thresh = num of bigrams allowes\n",
    "# # of the bigrams, are is their overlap in the rest of our words for a trigram?\n",
    "# trigram_model = gensim.models.Phrases(bigram_model[data_words], threshold = 50 )\n",
    "\n",
    "## create \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "\n",
    "# # fit bigram model \n",
    "# bigram = gensim.models.phrases.Phraser(bigram_model)\n",
    "# trigram = gensim.models.phrases.Phraser(trigram_model)\n",
    "\n",
    "# # instantia\n",
    "# data_bigrams = make_bigrams(data_words)\n",
    "# data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "# print(data_bigrams_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa2413-3d76-483d-86f5-c17f9d5f73f7",
   "metadata": {},
   "source": [
    "# Lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad23d56-f504-4321-838f-32dd000aa157",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0675716-d714-40ef-b6ba-26148abef88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postages=['NOUN','ADJ','VERB','ADV']):\n",
    "    \n",
    "    '''\n",
    "    allowed_postage : the parts of speach we want to keep [DEFAULT: 'NOUN','ADJ','VERB','ADV'] \n",
    "    '''\n",
    "    \n",
    "    # load in spacy sm web model \n",
    "    nlp = spacy.load('en_core_web_sm', diasble = ['parser','ner']) # computaltionally expensize aspects \n",
    "    texts_out = [] # output\n",
    "    \n",
    "    # for each post in the corpus\n",
    "    # iterate over texts\n",
    "    for text in texts:  \n",
    "        # creates spacy doc object containing vectorized contextual information like Parts of Speech (pos) \n",
    "        doc = nlp(text)\n",
    "        # list for holding lemmatized tokens\n",
    "        new_text = []\n",
    "        # iterate over each token\n",
    "        for token in doc:\n",
    "            # only keep the desired pos\n",
    "                if token.pos_ in allowed_postages and not token.is_stop and not token.is_punct and not token.like_num:\n",
    "                    # if str(token) not in stopwords:\n",
    "                        # reducing model complexity by reducing tokens to lemma_ \n",
    "                        new_text.append(token.lemma_)   \n",
    "                        # print(token.lemma_)\n",
    "\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f18955-c41e-4175-bf8a-bb37a2724231",
   "metadata": {},
   "source": [
    "# Similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f2cfcea-85c8-4d23-b9e3-57e4821b51aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train W2v with json file \n",
    "model = Word2Vec(segments, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "400e5efc-3823-48d1-9eaa-2afbf7aa6e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model \n",
    "model = Word2Vec.load('../models/demo.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2f99282-3a9d-4940-8525-1fa6c45fb413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sim_word(kwds, top_n):\n",
    "    \"\"\"\n",
    "    kwds : list of search terms\n",
    "    top_n: number of similar term to return \n",
    "    \n",
    "    returns: top_n most similar words for each of the terms in kwds\n",
    "    \"\"\"\n",
    "    # for each search term \n",
    "    for kwd in kwds:\n",
    "        try:\n",
    "            print(kwd.upper()) # show search term \n",
    "            res = model.wv.similar_by_word(kwd, topn=top_n) # find similar words\n",
    "            for term in res:\n",
    "                print(term) # show similar words\n",
    "        except KeyError:\n",
    "            print('[]\"Word not in vocabulary\"[]]')\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644eae3d-8bab-4115-b242-7514d3551c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_similarity(word, filepath):\n",
    "    model = KeyedVectors.load_word2vec_format(f'{filepath}', binary = False)\n",
    "    results = model.most_similar(positive =[word])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b83c608b-c003-4403-8a29-1a18641d9ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('shelter', 0.9994912147521973), ('downtown', 0.9993812441825867), ('those', 0.999374508857727), ('coalition', 0.9993677139282227), ('shelters', 0.9993603229522705), ('coliseum', 0.9993539452552795), ('hosting', 0.9993459582328796), ('bbq', 0.9993423223495483), ('monthly', 0.9993346333503723), ('campsite', 0.9993331432342529)]\n"
     ]
    }
   ],
   "source": [
    "gen_similarity('homeless','../data/word_vectors_demo_0.0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4f416-c924-4f37-a0b0-cb448ce1b60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15cc54a1-a323-417c-91d9-88cf7bbc75c1",
   "metadata": {},
   "source": [
    "## For creating a labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3c9d17b-a656-454d-aeae-5dbf0202d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_posts(post_tokens, kwds, label):    \n",
    "    '''\n",
    "    label: class label \n",
    "    kwds: search terms\n",
    "    post_tokens: tokenized corpus\n",
    "    \n",
    "    returns: labeled dataset containing target posts\n",
    "    '''\n",
    "    \n",
    "    train_data = []   \n",
    "    match = None\n",
    "    # if a post has one of our kwds\n",
    "    for word in kwds:\n",
    "        for post in post_tokens:\n",
    "            match = False \n",
    "            # only relevant posts \n",
    "            if word in post:\n",
    "                match = True\n",
    "            # if our keyword is in a post then match = True    \n",
    "            # attaches given class label to post\n",
    "            if match == True:\n",
    "                post = ' '.join(post)\n",
    "                train_data.append((post,label))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346214f4-e5ef-443a-9240-b1a002079715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0e307-e6d9-4203-b6b8-8de3c7a26476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import multiprocessing  # for using seperate core's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aea707-e72e-453e-adb5-19b577b22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_name, filepath):\n",
    "    with open (f'{filepath}', 'r', encoding = 'utf-8') as f:\n",
    "        texts = json.load(f)\n",
    "    sentence = texts \n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(\n",
    "            min_count = 2,\n",
    "            window = 2, # num of surrounding words to consider \n",
    "            # shape of a word vector\n",
    "            size = 500,  #size of vocab , dimensions of vocab                  \n",
    "            sample = 6e-5,\n",
    "            alpha =0.03, # error term\n",
    "            min_alpha=0.0007, # ??\n",
    "            negative = 20, # ??\n",
    "            workers = cores-1 # number of cores to train with  \n",
    "        )\n",
    "    \n",
    "    w2v_model.build_vocab(texts) # create the model vocabulary \n",
    "    w2v_model.train(texts, total_examples=w2v_model.corpus_count, epochs = 30) # train the model\n",
    "    w2v_model.save(f'../data/word_vec_model_{model_name}.model') # save the model for comparison \n",
    "    w2v_model.wv.save_word2vec_format(f'../data/word_vectors_{model_name}.txt') # save word vectors\n",
    "    # https://www.youtube.com/watch?v=1  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('work': conda)",
   "language": "python",
   "name": "python382jvsc74a57bd08696d3c135d391e67f0eae834eacd73bc5019c07a594ec3c835ec49a48ec6a4c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
